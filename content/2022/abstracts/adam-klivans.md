It has been known for decades that a polynomial-size training sample suffices
for core tasks in supervised learning.  Many theoretical results, however,
indicate that these learning tasks are computationally intractable.  Can we find
new algorithms to circumvent these hardness results?  As a case study, we
consider the fundamental problem of training neural networks.  We present a new
algorithm showing that even deep ReLU networks are fixed parameter tractable, a
result that provably cannot be obtained using gradient descent. 