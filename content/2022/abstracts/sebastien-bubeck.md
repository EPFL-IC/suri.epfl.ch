The discovery of the transformer architecture was a paradigm shifting event for
deep learning. However, these architectures are arguably even harder to
understand than say convolutional neural networks. In this work we propose a
synthetic task, called LEGO, to probe the inner workings of transformers. We
obtain some insights on multi-head attention, the effect of pretraining, as well
as overfitting issues. Joint work with Yi Zhang, Arturs Backurs, Ronen Eldan,
Suriya Gunasekar, and Tal Wagner.