Abstract: SGD is a workhorse for optimization and thus for statistics and
machine learning, and it is well understood in low dimensions. But understanding
its behavior in very high dimensions is not yet a simple task, and is a work in
progress. We describe here interesting and new regimes for the limiting
dynamics of  "summary statistics" for SGD in high dimensions. These regimes may
differ from the expected one given by the usual wisdom in finite dimensions,
i.e. the population gradient flow. We find that a new corrector term may be
needed and that the phase portrait of these dynamics is quite complex and
substantially different from what would be predicted using the classical
low-dimensional approach, including for simple tasks, like Tensor PCA, or simple
XOR classification. A related picture emerged rin the recent work done at EPFL
on two-layers networks by Veiga, Stephan, Loureiro, Krzakala and Zdeborova.

Joint work with Reza Gheissari (UC Berkeley) and Aukosh Jagannath (Waterloo)